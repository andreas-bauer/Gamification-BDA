---
title: "Gamification BDA"
execute:
  echo: true
---

## Introduction

Software testing is an integral part of software development.
Developers incorporate tests from low-level unit tests to high-level system tests and GUI-based tests.
GUI-based tests can verify a system's behavior through interactions with its GUI the same way a user would.
One approach to increase the engagement and motivation of people performing a task is Gamification, where elements originated in games are applied in a non-gaming context.
In the context of exploratory GUI-based testing, Gamification could improve testers' engagement and therefore improve the outcome of test activities.

Coppola et al. [1] conducted an experiment with 144 participants to investigate how Gamification would impact the effectiveness and efficiency of exploratory GUI testing.

In this project, I perform a (re-)analysis of the impacts of Gamification for exploratory GUI testing using Bayesian Data Analysis (BDA).
Input for this analysis is the [replication package](https://figshare.com/projects/GamificationReplicationPackage/127202) from the experiment [1].

## Prerequisites

```{r}
suppressPackageStartupMessages(library(dagitty))
suppressPackageStartupMessages(library(rethinking))
suppressPackageStartupMessages(library(magrittr))
suppressPackageStartupMessages(library(tidyverse))
```


## Directed Acyclic Graph (DAG)

A DAG helps us to understand the relationships between the different variables visually.

- **G**: Gamification approach was used; value = {true, false}
- **EJ**: Experience in Java; value = {"<1 year", "1-3 years", ">3 years"}
- **EW**: Experience in web development; value = {"<1 year", "1-3 years", ">3 years"}
- **ET**: Experience in software testing; value = {"<1 year", "1-3 years", ">3 years"}
- **W**: Inspected widgets
- **B**: Identified bugs
- **COV**: Test coverage
- **EF**: Effectiveness of exploratory testing
- **AW**: Average number of widgets on a page

```{r}
dag <- dagitty("dag {
G -> W
G -> B
EJ -> W
EJ -> B
EW -> W
EW -> B
ET -> W
ET -> B
W -> COV
B -> COV
AW -> COV
COV -> EF
}")
drawdag(dag)
```

Assumptions:

- G -\> W: Using Gamification encourage testers to inspect more widgets
- G -\> B: Using Gamification encourage testers to identify more bugs
- EJ -\> W: Experience in the used programming language will improve the inspection rate of widgets
- EJ -\> B: Experience in the used programming will identify more bugs
- EW -\> W: Experience in the technology of the SUT improves the inspection rate of widgets
- EW -\> B: Experience in the technology of the SUT improves the identification of bugs
- ET -\> W: Experience in software testing improves the inspection rate of widgets
- ET -\> B: Experience in software testing improves the identification of bugs
- W -\> COV: More inspected widgets increase the test coverage
- B -\> COV: More identified bugs increase the test coverage
- AW -\> COV: Average number of widgets on a page influence the test coverage of exploratory tests
- COV -\> EF: More test coverage improves the effectiveness of exploratory tests

### Building blocks

A causal DAG is always built using one or more of the four types of relations: Fork, Pipe, Collider, and Descendant.

**Fork**: In a fork relationship, a variable (G) is the cause of B and W, e.g., `B <- G -> W`. Here B and W stay independent

**Pipe**: In a pipe, variables influence the next element in the pipe, e.g., `B -> COV -> EF`. B influences COV, which influences EF. Conditioning on COV would block the information flow between B and EF.

**Collider**: In this DAG, an example of a collider is the `EJ -> W <- EW`.
A relationship between EF and WE only appears if you condition on W; otherwise, there will be no association between EJ and WE.

**Descendent**: The DAG consists of multiple descendent relationships, e.g., `AW -> COV <- B; COV -> EF`.
A descendent (EF) is influenced by another variable (COV).

## DAG simpified version

The presented DAG would require a multi-level model and a general more complex approach to investigate the impact of Gamification on the effectiveness of tests.
For this reason, I simplified the DAG as a starting point for the analysis.
In the following DAG, the amount of inspected widgets (W) serves as a proxy to understand the impact of Gamification.

```{r}
dag <- dagitty("dag {
G -> W
EJ -> W
EW -> W
ET -> W
}")
drawdag(dag)
```

## Estimating probability distributions

`G ~ beta-binomial(n, p, θ)`

`EJ, WE, ET ~ log-normal(µ, σ)`

`W   ~ possion(λ)`
``

## Import data from the replication package

Load raw data from replication package.

```{r}
demographicRaw <- read.csv(file = 'data/demographic.csv', header = TRUE, sep = ';', nrows=152)
withGameRaw <- read.csv(file = 'data/gamified_sessions.csv', header = TRUE, sep = ';', nrows=152)
withoutGameRaw <- read.csv(file = 'data/non_gamified_sessions.csv', header = TRUE, sep = ';', nrows=152)
```

Cleanup demographic data by replacing string representations of the experience levels with ordered categorical numbers (1,2, and 3).
The differences between categorical numbers are not equal, e.g., a person with 3 years of experience with testing will not result in 3 times better outcome compared to one with 1 year of experience.
```{r}
demographic <- subset(demographicRaw, select = c(Students.ID, expertise.in.java, expertise.in.sw.testing, expertise.in.web.app.development))

names(demographic) <- c("studentId", "expJava", "expTest", "expWeb")

expLessThan1Year = 1
expBetween1and3Years = 2
expMoreThan3Years = 3

expAsNumeric <- Vectorize(vectorize.args = "expLevel", FUN = function(expLevel) {
  experience = tolower(expLevel)
  result = switch(
    experience,
    "less than one year"=expLessThan1Year,
    "between one year and three years"=expBetween1and3Years,
    "more than three years"=expMoreThan3Years,
    0
  )
})

# transform strings to numerical values
demographic[2:4] <- lapply(demographic[2:4], expAsNumeric)

# transform numerical values to categorical values
demographic[2:4] <- lapply(demographic[2:4], factor)
```

```{r}
withGame <- subset(withGameRaw, select = c(Students.ID, X..Interactions.G))

names(withGame) <- c("studentId", "widgetsInspected")
withGame['gamification'] <- 1

withoutGame <- subset(withoutGameRaw, select = c(Students.ID, X..Interactions))
names(withoutGame) <- c("studentId", "widgetsInspected")
withoutGame['gamification'] <- 0

game_merged <-rbind(withGame, withoutGame)
game_merged <- merge(game_merged, demographic, by="studentId")
```

## Estimating model parameters

Bases on my understanding of the field of GUI-based testing, I estimate average values for the parameters, which makes those weakly informative priors.

```{r}
set.seed(2023)
N <- 100
alpha <- rlnorm(N, 5, 1)

for (i in i:N)
  curve(dlnorm(x,4,1), from = 0, to=N*2, n=N, xlab="N", ylab = "dlnorm(µ, σ)",  col = "blue")
  curve(dlnorm(x,5,1), from = 0, to=N*2, n=N,  col = "red", add = TRUE)
  curve(dlnorm(x,4.6,0.6), from = 0, to=N*2, n=N,  col = "orange", add = TRUE)
```

## References

[1] Coppola, R., Fulcini, T., Ardito, L., Torchiano, M. & Alégroth, E., Gamification: the next Silver Bullet for Exploratory GUI Testing? (in revision)

[2] McElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan.

